{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDDfV25AWFpC"
   },
   "source": [
    "# WEP24-MLB: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkEcMGl8WFpN"
   },
   "source": [
    "Textual data is everywhere:\n",
    "* Websites (e.g., news), social media (e.g., twitter), databases (e.g., doctors’ notes), digital scans of printed materials, …\n",
    "* Applications in industry: search, machine translation, sentiment analysis, question answering, …\n",
    "* Applications in science: cognitive modelling, understanding bias in language, automated systematic literature reviews, …\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSFm1446WFpU"
   },
   "source": [
    "## Regular Expressions\n",
    "\n",
    "Using the `re` library, run some of the examples from the slides. Try using the finctions `re.search()`, `re.match()`, `re.findall()`. Compare these functions with `str.startswith()` and `str.endswith()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uytzHV-dWFpW"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "'''\n",
    "TODO: given the following data, try using some regex queries \n",
    "e.g. finding the words that start with 'a' and has at least two characters\n",
    "or words that contain specific set of characters\n",
    "'''\n",
    "text = ['application', 'apple', 'banana', 'pear']\n",
    "matchings = [x for x in text if re.search('^a.', x)]\n",
    "print(matchings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZM19VPqWFpX"
   },
   "outputs": [],
   "source": [
    "matchings = [x for x in text if x.startswith('a')]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zpQNbkZWFpX"
   },
   "outputs": [],
   "source": [
    "r1 = re.compile(r\".a$\")\n",
    "matchings = [x for x in text if r1.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvkNWmopWFpY"
   },
   "outputs": [],
   "source": [
    "r1 = re.compile(r\"ap|an\")\n",
    "matchings = [x for x in text if r2.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_p883rn2WFpY"
   },
   "outputs": [],
   "source": [
    "r2 = re.compile(r\"Bat[wo]man\")\n",
    "text2 = ['Batman', 'Batoman', 'Batwoman', 'Batwman', 'Batwowoman']\n",
    "matchings = [x for x in text2 if r2.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhhj4gfY6hhz"
   },
   "source": [
    "It doesn't work as you were expecting, right? Can you do something different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PqeKHmZf8OJY"
   },
   "outputs": [],
   "source": [
    "r2 = re.compile(r\"Bat(wo)?man\")\n",
    "text2 = ['Batman', 'Batoman', 'Batwoman', 'Batwman', 'Batwowoman']\n",
    "matchings = [x for x in text2 if r2.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00rKVvAoWFpY"
   },
   "outputs": [],
   "source": [
    "r3 = re.compile(r\"[b-f](at|ot)\")\n",
    "text3 = ['bat', 'cat', 'hat', 'eat', 'nat', 'oat', 'pat', 'Pat', 'ot', 'bot', 'got', '-at', '&at']\n",
    "matchings = [x for x in text3 if r3.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrG5GxaSWFpZ"
   },
   "outputs": [],
   "source": [
    "r4 = re.compile(r\"^color$\")\n",
    "text4 = ['color', 'colour', 'colourhat', 'colormat', 'colournat', '123oat', 'pat', 'Pat', 'ot', 'bot', 'mot', '-at', '&at']\n",
    "matchings = [x for x in text4 if r4.search(x)]\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34t__q9QWFpZ"
   },
   "outputs": [],
   "source": [
    "r5 = re.compile(r\"[0-9]{2}\")\n",
    "text5 = 'My 2 favorite numbers are 19 and 4222'\n",
    "matchings = r5.findall(text4)\n",
    "matchings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwzOQ1CsWFpa"
   },
   "outputs": [],
   "source": [
    "text6 = 'You want to find sub-words that have three letters'\n",
    "matchings = re.findall('[(a-z)|(A-Z)]{3}', text6)\n",
    "matchings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fShEQd38-b47"
   },
   "source": [
    "Try the same text but remove the spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODcURfTK-h_S"
   },
   "outputs": [],
   "source": [
    "text = 'You want to find sub-words that have three letters'\n",
    "nst = text.replace(' ', '')\n",
    "matchings = re.findall('[(a-z)|(A-Z)]{3}', nst)\n",
    "matchings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFGa2m9mWFpa"
   },
   "source": [
    "## Textual Data Preprocessing\n",
    "\n",
    "We will start by installing the `nltk` library and the sett of the stop-wrods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLPDF5Dw41EJ"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2IlOT0j45jK"
   },
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQP34w7qWFpQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aR8hw1j9af7"
   },
   "source": [
    "We perform the same tasks that were discussed in the slides including tokenization, stemming, and removing stop-word and punctuation mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HYFuS9oWFpb"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfqyyIZYWFpb"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "'''\n",
    "TODO: Tokenize the text and display the set of tokens ..\n",
    "consider the tokens are separated by spaces and special characters.\n",
    "'''\n",
    "text = 'Morning and afternoon are parts of the day.'\n",
    "word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlNWGTrSWFpb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Tokenize the text and display the set of tokens ..\n",
    "consider the tokens are separated by spaces and special characters.\n",
    "\n",
    "Use 're' library this time.\n",
    "'''\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S')\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYFPFotuWFpc"
   },
   "source": [
    "### Stop-word and Punctuation Marks Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NFHOL8LWFpd"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "# text with no stop words\n",
    "nsw_text = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "# text with no punctuation marks\n",
    "filtered_text = [word for word in nsw_text if re.sub(r'[^\\w\\s]', '', word)]\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdKSE4qiWFpd"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeZwyLlzWFpe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: run porter's stemmer to find the root of each of the words in the sentence\n",
    "'''\n",
    "text = \"Morning and afternoon are parts of the day.\"\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(w) for w in filtered_text]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Stemmed text:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BjFdxx_WFpe"
   },
   "source": [
    "## Textual Data Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJClmpaBWFpe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Create a dictionary that contains the term and its frequency in the document.\n",
    "\n",
    "Display the TF matrix\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "D1 = 'the cat sits on the bed'\n",
    "D2 = 'the dog sits on the bed'\n",
    "corpus = [D1,D2]\n",
    "rows = ['D1', 'D2']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "frequencies = X.toarray()\n",
    "dtm = pd.DataFrame(frequencies, columns = terms, index = rows)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMbh44yNWFpf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: repeat the same exercise but remove the stop words this time.\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "D1 = 'the cat sits on the bed'\n",
    "D2 = 'the dog sits on the bed'\n",
    "corpus = [D1,D2]\n",
    "rows = ['D1', 'D2']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "dtm = pd.DataFrame(X, columns = terms, index = rows)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmicN3_ZWFpf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Create a dictionary that contains the terms and their TF-IDF scores.\n",
    "\n",
    "Display the TF-IDF matrix\n",
    "'''\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "D1 = 'the cat sits on the bed'\n",
    "D2 = 'the dog sits on the bed'\n",
    "corpus = [D1,D2]\n",
    "rows = ['D1', 'D2']\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "dtm = pd.DataFrame(X, columns = terms, index = rows)\n",
    "dtm = dtm.round(2)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8LuLzhtWFpf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Create a dictionary that contains the terms and their word-word\n",
    "co-occurrence matrix (WWCM) scores.\n",
    "\n",
    "Display the WWCM matrix\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "D1 = 'the cat sits on the bed'\n",
    "D2 = 'the dog sits on the bed'\n",
    "corpus = [D1,D2]\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts\n",
    "cv = CountVectorizer(ngram_range=(1,1), stop_words = 'english')\n",
    "# matrix of token counts\n",
    "X = cv.fit_transform(corpus)\n",
    "Xc = (X.T * X) # matrix manipulation\n",
    "Xc.setdiag(0) # set the diagonals to be zeroes as it's pointless to be 1\n",
    "names = cv.get_feature_names_out()\n",
    "df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usj8cHLlI8qT"
   },
   "source": [
    "## Word Embedding\n",
    "### gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrFuWRKOfeHp"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need also to preprocess the text using `nltk` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAPnpw7VgclV"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download text to be used for training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1Elkmmg_ylJ"
   },
   "outputs": [],
   "source": [
    "!wget -c https://www.gutenberg.org/files/11/11-0.txt\n",
    "!mv 11-0.txt sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2r973AmWFpg"
   },
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Reads text file\n",
    "sample = open(\"sample_data/11-0.txt\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "vs = 300        # The size of the word vector\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    data.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "m1 = gensim.models.Word2Vec(data, min_count=1, vector_size=vs, window=5)\n",
    "\n",
    "# Compute the similarity and print it\n",
    "print(\"CBOW('alice', 'computer') = \", m1.wv.similarity('alice', 'computer'))\n",
    "print(\"CBOW('alice', 'wonderland') = \", m1.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "\n",
    "# Create Skip Gram model\n",
    "m2 = gensim.models.Word2Vec(data, min_count=1, vector_size=vs,\twindow=5, sg=1)\n",
    "\n",
    "# Compute the similarity and print it\n",
    "print(\"SG('alice', 'computer') = \", m2.wv.similarity('alice', 'computer'))\n",
    "print(\"SG('alice', 'wonderland') = \", m2.wv.similarity('alice', 'wonderland'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dd7I5Ovq5Y1c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Use the models most_similar() function to print the words that\n",
    "are very close to a given word such as 'beginning', 'computer'\n",
    "'''\n",
    "m1.wv.most_similar(\"beginning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_B4F_rnQr-o"
   },
   "outputs": [],
   "source": [
    "m1.wv.most_similar(\"computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AriI-IsuJv1M"
   },
   "source": [
    "Before trying to print the vector representation of a word, you may think of reducing the size of the vector in the training step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRtQM0wVMnV8"
   },
   "outputs": [],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# Reads text file\n",
    "sample = open(\"sample_data/11-0.txt\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "vs = 5        # The size of the word vector\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    data.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "m1 = gensim.models.Word2Vec(data, min_count=1, vector_size=vs, window=5)\n",
    "\n",
    "# Compute the similarity and print it\n",
    "print(\"CBOW('alice', 'computer') = \", m1.wv.similarity('alice', 'computer'))\n",
    "print(\"CBOW('alice', 'wonderland') = \", m1.wv.similarity('alice', 'wonderland'))\n",
    "\n",
    "\n",
    "# Create Skip Gram model\n",
    "m2 = gensim.models.Word2Vec(data, min_count=1, vector_size=vs,\twindow=5, sg=1)\n",
    "\n",
    "# Compute the similarity and print it\n",
    "print(\"SG('alice', 'computer') = \", m2.wv.similarity('alice', 'computer'))\n",
    "print(\"SG('alice', 'wonderland') = \", m2.wv.similarity('alice', 'wonderland'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKCcPwwdDoG0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Use the models get_vector() function to print the vector representation\n",
    "of a word that exist in the vocabulary of the model\n",
    "'''\n",
    "m1.wv.get_vector('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLfXHMAOOFy1"
   },
   "source": [
    "### gensim with pretrained models\n",
    "\n",
    "We will start by importing the downloader of the pretrained models and list the set of available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhddzhN3ODwL"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvbmTxDaOpBo"
   },
   "source": [
    "Let us try a small model `glove-twitter-25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vAeaCS0OoUh"
   },
   "outputs": [],
   "source": [
    "gtv = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i88GDmARQHWm"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Use the pretrained model's most_similar() function to print the words that\n",
    "are very close to a given word such as 'beginning', 'computer'\n",
    "Compare the results with those that were obtained using the model that we trained earlier.\n",
    "'''\n",
    "gtv.most_similar('beginning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU-Ew6cVQu6f"
   },
   "outputs": [],
   "source": [
    "gtv.most_similar('computer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzod5U_1JLUl"
   },
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dJl57_nAZwX"
   },
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwN7gi4KE3Y0"
   },
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0thD-H0tE6JP"
   },
   "outputs": [],
   "source": [
    "ftm1 = fasttext.train_unsupervised('sample_data/11-0.txt', minn=2, maxn=5, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1Li6l7DTw_a"
   },
   "outputs": [],
   "source": [
    "ftm1.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPDtv05sUwWw"
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.reduce_model(ftm1, 5)\n",
    "ftm1.get_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgdy6CZ8FfEL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: print the vector representation of the word \"language\"\n",
    "'''\n",
    "ftm1.get_word_vector(\"language\").round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfR0ie-gRrC5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: display the 5, 10 and 20 closest neighbors to the word \"language\"\n",
    "'''\n",
    "n = 5\n",
    "ftm1.get_nearest_neighbors(\"language\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEtJ5yJeT6jL"
   },
   "source": [
    "### fastText with pretrained models\n",
    "The downloading step will take long time as you will need to download nearly 4.5 GB compressed file and it will be extracted to 7.24 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaE394CmLddq"
   },
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5OwRNdWTmP3"
   },
   "source": [
    "Now, try to find the closest word to the word `computer` or `beginning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45A0Taz4RQDx"
   },
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ntrn2CCRuOc"
   },
   "outputs": [],
   "source": [
    "ft.get_word_vector('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9YqopRZI2hi"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "### Using Transformers\n",
    "\n",
    "A nice tuttorial for using trnasformers for sentiment analysis can be found [here](https://huggingface.co/blog/sentiment-analysis-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl1eTNhhWFpg"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "data = [\"KAUST was established to become world class university\", \"KAU is an old university\"]\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6ekMQWKGadj"
   },
   "source": [
    "### Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FcRh0B7sU5i"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9svLZyNLavEa"
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52xaVTtjVrrf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: display the 'polarity_scores' of the following two sentences and \n",
    "compare the results with those obtained using the transformers\n",
    "'''\n",
    "data = [\"KAUST was established to become world class university\",\n",
    "        \"KAU is an old university\"]\n",
    "[sia.polarity_scores(x) for x in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think that the results are different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHEAtcn3sfAv"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Try with these sentences.\n",
    "'''\n",
    "data = [\"KAUST is a good university\",\n",
    "        \"Students are feeling happy\"]\n",
    "[sia.polarity_scores(x) for x in data]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
