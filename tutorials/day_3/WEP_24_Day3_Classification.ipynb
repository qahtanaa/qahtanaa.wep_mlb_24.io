{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcmepKAf7t_d"
   },
   "source": [
    "# WEP24-MLB: Classification and Algorithmic Fairness\n",
    "\n",
    "In the first part we will try to classify handwritten digits by using multiple Machine Learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHi-lCQx96hB"
   },
   "source": [
    "## 1. Classifying Handwritten Digits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpGOCc2W7t_s"
   },
   "source": [
    "**1.1 Looking at the data**\n",
    "\n",
    "Start by importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH5nPaT87t_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxs0XGca7t_u"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "mnist_data = pd.read_csv('sample_data/mnist_train_small.csv').values\n",
    "mnist_data = pd.DataFrame(mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aI3R-0UR7t_w"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Find how many examples are in the dataset.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28itbSt0j49V"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Show the the first 5 rows of the data using the head()-function\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yIV4j1Lltfe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Print one instance from the data\n",
    "'''\n",
    "np.array(mnist_data.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyAji3Cy7t_0"
   },
   "outputs": [],
   "source": [
    "# Show the number of times each digit is in the data.\n",
    "mnist_data[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNIUYVNI7t_1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we split the the data into the labels and the features\n",
    "digits = mnist_data[0]\n",
    "pixels = mnist_data.drop(0, axis=1)\n",
    "print(\"Shape of labels = \", np.shape(digits))\n",
    "print(\"Shape of features = \", np.shape(pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiBRi3uv7t_2"
   },
   "outputs": [],
   "source": [
    "# This output shows you the label and the pixel values of an example image.\n",
    "plt.imshow(pixels.loc[0].values.reshape(28,28))\n",
    "plt.show()\n",
    "print(\"Label = \", digits[0])\n",
    "print(\"Pixel values = \", pixels.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5adFa-TC7t_3"
   },
   "source": [
    "**1.2 Splitting the data into train and test sets**\n",
    "\n",
    "In the next step we will split the data into a train set and a test set. We will use sklearn's function called 'train_test_split'. We want the train set to be 70% of the data, and the test set 30%. Look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and try to find out what needs to be filled in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew_3xesg7t_4"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOH2g0R37t_5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: check if the datasets have the intended number of records?\n",
    "Use np.shape \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVgqR1CO7t_5"
   },
   "source": [
    "**1.3 Logistic Regression**\n",
    "\n",
    "Now we can create a Logistic Regression classifier. We will use LogisticRegression(solver='saga'). You can leave other parameters to their default values. Instantiate the classifier, fit the model and make a prediction.\n",
    "\n",
    "You can look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) for more information .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjBMYrGd7t_6"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "'''\n",
    "TODO: Instantiate the classifier, fit the model and make a prediction.\n",
    "'''\n",
    "clf_LR = LogisticRegression(solver='saga')\n",
    "clf_LR.fit(X_train, y_train)\n",
    "pred_LR = clf_LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MkKzCPhIkca"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Detailed output of the LR model\n",
    "'''\n",
    "n = 4012\n",
    "y_new = y_test.reset_index()\n",
    "p_classes = clf_LR.predict_proba(X_test)\n",
    "print(y_new.loc[n])\n",
    "print([round(a, 4) for a in p_classes[n]])\n",
    "print(pred_LR[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z8c31gJ7t_7"
   },
   "source": [
    "To evaluate the performance of the classifier we can look at the accuracy. Look at the documentation for the [accuracy score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfIqf-eL7t_7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Calculate the accuracy\n",
    "accuracy_LR = accuracy_score(y_test, pred_LR)\n",
    "print(\"Accuracy = \", round(accuracy_LR * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJWZJragiwhA"
   },
   "source": [
    "Plot the confusion matrix to see how often the digits get classified correctly, and what mistakes were made. (You can use [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics)). Try to determine which digits often get mistaken for each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipFtBP-hzw3O"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Create the confusion matrix\n",
    "cm_LR = confusion_matrix(y_test, pred_LR)\n",
    "\n",
    "# Which digits get often confused for each other?\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_LR)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS_NUENZ0VC2"
   },
   "source": [
    "**1.4 Random Forest**\n",
    "\n",
    "Similar to the Logistic Regression classifier, we can make a [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNO4ahX77t_8"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "'''\n",
    "TODO: Instantiate the classifier, fit the model and make a prediction.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsY_wafE7t_8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Compute the accuracy of the random forest model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0-VOQGr1ibW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Create and display the confusion matrix\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n88wb0I-110t"
   },
   "source": [
    "**1.5 Neural Network**\n",
    "\n",
    "Finally, we will train a neural network for the classification task. We will use a [Multi-Layer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwMMszLe7t_9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "'''\n",
    "TODO: Instantiate the classifier, fit the model and make a prediction.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxB8eEUC7t_9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Compute the accuracy of the neural network model.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNXjtc-D2Xyv"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Create and display the confusion matrix\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1YqSNRe7t_9"
   },
   "source": [
    "## 2. Measuring the bias in datasets\n",
    "\n",
    "In this part we will look at the GERMAN dataset. More information about the dataset can be found [here](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heiiDrOe8FD7"
   },
   "source": [
    "**2.1 Loading the data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqmC3zi47t_-"
   },
   "outputs": [],
   "source": [
    "!pip install aif360\n",
    "!pip install 'aif360[LawSchoolGPA]'\n",
    "!pip install 'aif360[AdversarialDebiasing]'\n",
    "'''\n",
    "There are other fairness algorithms in the library fairlearn.\n",
    "You can install that library using:\n",
    "!pip install fairlearn\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbDx5OXo8TQc"
   },
   "outputs": [],
   "source": [
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-PjXbKz96hQ"
   },
   "outputs": [],
   "source": [
    "from aif360.datasets import GermanDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqXV-46Wc_CP"
   },
   "source": [
    "We need to check the version of the Python that we are using before downloading the data. The dataset should be copied into the correct in the virtual enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzoPxQbm_ydt"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l75oTUTEdU0t"
   },
   "source": [
    "The following commands work on Google Colab environment. If you are using a different IDE, you need to figure out how to download the dataset and copy it to the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2He4xOv7t_-"
   },
   "outputs": [],
   "source": [
    "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
    "!wget -c https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc\n",
    "!mv german.data /usr/local/lib/python3.10/dist-packages/aif360/data/raw/german\n",
    "!mv german.doc /usr/local/lib/python3.10/dist-packages/aif360/data/raw/german"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wk93vgUXrYZE"
   },
   "source": [
    "**2.2 Looking at the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzw76eNVCF-t"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read the dataset\n",
    "'''\n",
    "german_data = GermanDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyW4AhOT-TBc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "What is the label, and what does the label mean?\n",
    "'''\n",
    "label = german_data.label_names\n",
    "fav = german_data.favorable_label\n",
    "unfav = german_data.unfavorable_label\n",
    "print(\"Label = \", label)\n",
    "print(\"Favorable label = \", fav)\n",
    "print(\"Unfavorable label = \", unfav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l68g9Rje96hR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We extract a datframe from the dataset for better visualization\n",
    "'''\n",
    "german_df = german_data.convert_to_dataframe()\n",
    "german_df = german_df[0]\n",
    "german_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsW4Ja10rxiH"
   },
   "source": [
    "In the data, sex is divided in 0 = Female, and 1 = Male. Age is divided into 0 <= 25, and 1 > 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8LoB7Mio0Au"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's look at the distribution of the data and the class labels.\n",
    "What do you observe?\n",
    "'''\n",
    "german_data.convert_to_dataframe()[0].groupby(['age','sex','credit'])['credit'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9wgfbDso0Ro"
   },
   "outputs": [],
   "source": [
    "priv_group   = [{'age': 1, 'sex' : 1}]  # Men with age > 25\n",
    "unpriv_group = [{'age': 0, 'sex': 0}]  # Women with age <= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOS14Xezo_cy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The base rate tells you the percentage of positive outcomes. What do you notice?\n",
    "'''\n",
    "metric_orig = BinaryLabelDatasetMetric(german_data,\n",
    "                                       unprivileged_groups=unpriv_group,\n",
    "                                       privileged_groups=priv_group)\n",
    "print(\"Percentage of positive outcomes for the unprivileged group = \",\n",
    "      round(metric_orig.base_rate(False) * 100, 2))\n",
    "print(\"Percentage of positive outcomes for the privileged group = \",\n",
    "      round(metric_orig.base_rate(True) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCO3CdtLuYzr"
   },
   "source": [
    "**2.3 Training a classifier**\n",
    "\n",
    "We have looked at the original data. But what happens if we train a classifier on this biased dataset?\n",
    "\n",
    "Start by splitting the data in a train and a test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAdMNQgCvfik"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = german_data.split([0.8], shuffle=True)\n",
    "X_train = train_data.features\n",
    "y_train = train_data.labels.ravel()\n",
    "X_test = test_data.features\n",
    "y_test = test_data.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRmOAo6v8FtY"
   },
   "source": [
    "We will look at the base rates in the train and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prz6qetZ3rmz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Check the base rate for the training data\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxMZYB9w3fwe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Check the base rate for the test data\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwbtt0UuvwGR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: train a LogisticRegression classifier\n",
    "'''\n",
    "clf_LR_german = LogisticRegression()\n",
    "clf_LR_german.fit(X_train, y_train)\n",
    "pred_german = test_data.copy(deepcopy = True)\n",
    "pred_german.labels = clf_LR_german.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3K2ZWwX2wFeT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate the model\n",
    "'''\n",
    "print('Accuracy = ', accuracy_score(y_test, pred_german.labels))\n",
    "cm = confusion_matrix(y_test, pred_german.labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EdqXWyxzOoS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now look at the percentage of positive outcomes for both groups in the predictions.\n",
    "What happened?\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXVVi3yfuFJ-"
   },
   "source": [
    "## 3. Measuring the bias in classifier's outcome\n",
    "\n",
    "In the final part we will look at the fairness of the classifier trained on the GERMAN dataset. We create a [ClassificationMetric](https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html) object, and use its methods to calculate multiple fairness measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ImyOep_7b_o"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We create a ClassificationMetric class to measure the fairness of\n",
    "the classifier's outcomes\n",
    "'''\n",
    "fairness_metric = ClassificationMetric(test_data, pred_german,\n",
    "                                       unprivileged_groups= unpriv_group,\n",
    "                                       privileged_groups= priv_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-J1KyVhuQdz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Calculate the Statistical Parity difference and the Disparate Impact (ratio)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ktPzZ-462nx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate the True Positive Rate for the privileged and the unprivileged group,\n",
    "and their differences\n",
    "'''\n",
    "TPR_priv = fairness_metric.true_positive_rate(privileged=True)\n",
    "TPR_unpriv = fairness_metric.true_positive_rate(privileged=False)\n",
    "TPR_difference = fairness_metric.true_positive_rate_difference()\n",
    "print(\"TPR (priv_group) =\", round(TPR_priv, 2)) #TPR for privileged group\n",
    "print(\"TPR (unpriv_group) =\", round(TPR_unpriv, 2)) #TPR for unprivileged group\n",
    "print(\"TPR difference =\", abs(round(TPR_difference, 2))) #TPR difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcOUBCRo7Aca"
   },
   "outputs": [],
   "source": [
    "#Use the consistency metric to calculate individual fairness\n",
    "consistency = fairness_metric.consistency()\n",
    "print(\"The consistency is an individual fairness metric.\")\n",
    "print(\"It measures how similar the labels are for similar instances = \",\n",
    "      round(consistency[0] * 100, 2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
